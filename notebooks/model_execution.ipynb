{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prerequisites for Running the Models**\n",
    "\n",
    "Before running this notebook, ensure the following steps are completed:\n",
    "\n",
    "1. **Preprocess the Data**:  \n",
    "   - The raw datasets must be cleaned, formatted, and preprocessed.  \n",
    "   - Use the `data_preprocessing.ipynb` notebook to perform preprocessing.  \n",
    "   - This notebook requires the cleaned datasets for both the tweets and the historical stock data.\n",
    "   - Link to the preprocessing notebook: [data_preprocessing.ipynb](./data_preprocessing.ipynb).\n",
    "\n",
    "2. **Python Environment**:  \n",
    "   - Ensure Python 3.8 or higher is installed.\n",
    "\n",
    "3. **Install Required Dependencies**:  \n",
    "   - Run the following command to install all necessary dependencies:  \n",
    "     ```bash\n",
    "     pip install -r requirements.txt\n",
    "     ```\n",
    "    \n",
    "### **Importing Required Modules**\n",
    "\n",
    "The following code block imports all necessary Python libraries used for:\n",
    "- Data handling, preprocessing, and visualization.\n",
    "- Time-series and machine learning models (ARIMA, XGBoost, LSTM).\n",
    "- Sentiment analysis and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Modules used for general utilities\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pickle import load, dump\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "# Time Series Modelling\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    ")\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Deep learning models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting Tweets for Specific Stocks**\n",
    "The following code demonstrates the process of selecting a specific stock (e.g., TSLA, AAPL, F) from the dataset and extracting the associated tweets. The goal is to analyze sentiment from these tweets for subsequent integration into the hybrid stock forecasting models.  \n",
    "Starting from this codeblock, all the codeblocks must be run several times to analyze more than one stock.\n",
    "\n",
    "#### Process Overview:\n",
    "\n",
    "1. **Select a Stock Name**:  \n",
    "   - The stock name (ticker symbol) must be specified in the code.\n",
    "   - This process needs to be repeated for each stock under analysis:  \n",
    "     - **TSLA** (high public discourse).  \n",
    "     - **AAPL** (moderate public discourse).  \n",
    "     - **F** (low public discourse).  \n",
    "\n",
    "2. **Fill in the Company Name:** Aside from the ticker symbol of the stock, the company name must also be specified in the code. This is essential in defining the labels to ensure the LLM can predict the correct sentiment for Tweets. For instance, for `TSLA` as the `stock_name`, the `company_name` would be `Tesla`\n",
    "\n",
    "\n",
    "#### Instructions for Repetition:\n",
    "- To repeat this process for another stock, simply change the value of the `stock_name` and `company_name` variables to the desired ticker symbol (e.g., \"AAPL\" or \"F\") and company name (e.g., \"Apple\" or \"Ford\") and rerun the code block.\n",
    "- Ensure the extracted tweet data is saved for each stock separately to avoid overwriting.\n",
    "\n",
    "### Importance\n",
    "- This step is critical for isolating the public sentiment related to individual stocks.  \n",
    "- Sentiment analysis results may vary significantly depending on the volume and context of tweets, making it essential to repeat this process for each stock individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37422, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-29 23:41:16+00:00</td>\n",
       "      <td>Mainstream media has done an amazing job at brainwashing people. Today at work, we were asked what companies we believe in &amp;amp; I said @Tesla because they make the safest cars &amp;amp; EVERYONE disagreed with me because they heard“they catch on fire &amp;amp; the batteries cost 20k to replace”</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-29 23:24:43+00:00</td>\n",
       "      <td>Tesla delivery estimates are at around 364k from the analysts. $tsla</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-29 23:18:08+00:00</td>\n",
       "      <td>3/ Even if I include 63.0M unvested RSUs as of 6/30, additional equity needed for the RSUs is 63.0M x $54.20 = $3.4B. If the deal closed tomorrow at $54.20, Elon would need $2.0B for existing shares plus $3.4B for RSUs, so $5.4B new equity. $twtr $tsla</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-29 22:40:07+00:00</td>\n",
       "      <td>@RealDanODowd @WholeMarsBlog @Tesla Hahaha why are you still trying to stop Tesla FSD bro! Get your shit together and make something better? Thats how companies work, they competed. Crying big old ass fart clown!</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-29 22:27:05+00:00</td>\n",
       "      <td>@RealDanODowd @Tesla Stop trying to kill kids, you sad deranged old man</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date  \\\n",
       "0  2022-09-29 23:41:16+00:00   \n",
       "1  2022-09-29 23:24:43+00:00   \n",
       "2  2022-09-29 23:18:08+00:00   \n",
       "3  2022-09-29 22:40:07+00:00   \n",
       "4  2022-09-29 22:27:05+00:00   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              Tweet  \\\n",
       "0  Mainstream media has done an amazing job at brainwashing people. Today at work, we were asked what companies we believe in &amp; I said @Tesla because they make the safest cars &amp; EVERYONE disagreed with me because they heard“they catch on fire &amp; the batteries cost 20k to replace”   \n",
       "1                                                                                                                                                                                                                              Tesla delivery estimates are at around 364k from the analysts. $tsla   \n",
       "2                                      3/ Even if I include 63.0M unvested RSUs as of 6/30, additional equity needed for the RSUs is 63.0M x $54.20 = $3.4B. If the deal closed tomorrow at $54.20, Elon would need $2.0B for existing shares plus $3.4B for RSUs, so $5.4B new equity. $twtr $tsla   \n",
       "3                                                                              @RealDanODowd @WholeMarsBlog @Tesla Hahaha why are you still trying to stop Tesla FSD bro! Get your shit together and make something better? Thats how companies work, they competed. Crying big old ass fart clown!   \n",
       "4                                                                                                                                                                                                                           @RealDanODowd @Tesla Stop trying to kill kids, you sad deranged old man   \n",
       "\n",
       "  Stock Name Company Name  \n",
       "0       TSLA  Tesla, Inc.  \n",
       "1       TSLA  Tesla, Inc.  \n",
       "2       TSLA  Tesla, Inc.  \n",
       "3       TSLA  Tesla, Inc.  \n",
       "4       TSLA  Tesla, Inc.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_name = 'TSLA'\n",
    "company_name = 'Tesla'\n",
    "all_tweets = pd.read_csv(r'../data/preprocessed_tweets.csv')\n",
    "df_tweets_stock = all_tweets[all_tweets['Stock Name'] == stock_name]\n",
    "print(df_tweets_stock.shape)\n",
    "df_tweets_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling Tweets**\n",
    "Conducting sentiment analysis of 1000s of tweets is too tedious and time-consuming of a task to undertake, therefore we are sampling 100 tweets as a subset of tweets to conduct sentiment analysis. Sampling ensures a consistent and manageable dataset while maintaining representativeness.\n",
    "\n",
    "#### **Process Overview**\n",
    "1. **Random Sampling:**\n",
    "If the total number of tweets in the dataset exceeds 100, a random sample of 100 tweets is selected.\n",
    "If there are fewer than 100 tweets, the entire population of tweets is used.\n",
    "\n",
    "2. **Reproducibility:**\n",
    "A fixed `random_state` (set to 42) ensures that the random sampling produces the same subset of tweets for reproducibility in sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_tweets_stock.shape[0] > 100:\n",
    "    df_tweets_stock_test = df_tweets_stock.sample(n=100, random_state=42)\n",
    "else:\n",
    "    df_tweets_stock_test = df_tweets_stock.sample(n=df_tweets_stock.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentiment Analysis using VADER & Zero Shot Text Classification**\n",
    "⚠️ **Warning:** Zero Shot Text Classification can potentially take hours to run. To visualize the results without running you may view the following images:\n",
    "\n",
    "* [Results for TSLA](../results/RMSE%20Table/Results%20TSLA.png)\n",
    "* [Results for AAPL](../results/RMSE%20Table/Results%20AAPL.png)\n",
    "* [Results for F](../results/RMSE%20Table/Results%20F.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize VADER sentiment analyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for indx, row in df_tweets_stock_test.iterrows():\n",
    "    try:\n",
    "        # Normalize the tweet text\n",
    "        sentence_i = unicodedata.normalize('NFKD', row['Tweet'])\n",
    "\n",
    "        # Get sentiment scores\n",
    "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
    "\n",
    "        # Assign sentiment scores to the DataFrame\n",
    "        df_tweets_stock_test.at[indx, 'Sentiment_score VADER'] = sentence_sentiment['compound']\n",
    "        df_tweets_stock_test.at[indx, 'Negative VADER'] = sentence_sentiment['neg']\n",
    "        df_tweets_stock_test.at[indx, 'Neutral VADER'] = sentence_sentiment['neu']\n",
    "        df_tweets_stock_test.at[indx, 'Positive VADER'] = sentence_sentiment['pos']\n",
    "\n",
    "    except TypeError:\n",
    "        print(df_tweets_stock_test.loc[indx, 'Tweet'])\n",
    "        print(indx)\n",
    "        break\n",
    "    \n",
    "# Define the thresholds\n",
    "threshold_positive = 1/3\n",
    "threshold_negative = -1/3\n",
    "\n",
    "# Add the Predicted Label column based on the Sentiment Score\n",
    "df_tweets_stock_test['Predicted Label LLM'] = df_tweets_stock_test['Sentiment Score LLM'].apply(\n",
    "    lambda x: 1 if x > threshold_positive else (-1 if x < threshold_negative else 0)\n",
    ")\n",
    "\n",
    "def predict_label(row):\n",
    "    if row['Positive VADER'] > row['Negative VADER'] and row['Positive VADER'] > row['Neutral VADER']:\n",
    "        return 1\n",
    "    elif row['Negative VADER'] > row['Positive VADER'] and row['Negative VADER'] > row['Neutral VADER']:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to each row\n",
    "df_tweets_stock_test['Predicted Label VADER'] = df_tweets_stock_test.apply(predict_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1586\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:994\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     TFSeq2SeqSequenceClassifierOutput,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-mnli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Define the candidate labels globally\u001b[39;00m\n\u001b[0;32m      3\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive about \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative about Tesla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral about Tesla\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:258\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 258\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1577\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1576\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1577\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1576\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1574\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1576\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1577\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1588\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1591\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "# Define the candidate labels globally\n",
    "candidate_labels = [f\"positive about {company_name}\", f\"negative about {company_name}\", \"neutral about {company_name}\"]\n",
    "\n",
    "def classify_sentiment_about_tesla(tweet):\n",
    "    # Perform zero-shot classification\n",
    "    result = pipe(tweet, candidate_labels=candidate_labels)\n",
    "\n",
    "    # Extract the scores for each sentiment\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    neutral_score = 0\n",
    "\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        if label == \"positive about Tesla\":\n",
    "            positive_score = score * 2 / 3\n",
    "        elif label == \"negative about Tesla\":\n",
    "            negative_score = score  * 2 / 3\n",
    "        elif label == \"neutral about Tesla\":\n",
    "            neutral_score = score  * 2 / 3\n",
    "\n",
    "    # Determine the overall sentiment based on the highest score\n",
    "    if positive_score >= negative_score and positive_score >= neutral_score:\n",
    "        sentiment_score =   1 / 3 + positive_score\n",
    "    elif negative_score >= positive_score and negative_score >= neutral_score:\n",
    "        sentiment_score = -1 / 3 - negative_score\n",
    "    else:\n",
    "        sentiment_score =  positive_score - negative_score\n",
    "\n",
    "    return sentiment_score, negative_score, neutral_score, positive_score\n",
    "\n",
    "%%time\n",
    "df_tweets_stock_test[['Sentiment Score LLM', 'Negative LLM', 'Neutral LLM', 'Positive LLM']] = df_tweets_stock_test['Tweet'].apply(\n",
    "    lambda tweet: pd.Series(classify_sentiment_about_tesla(tweet))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize RoBERTa sentiment pipeline and VADER sentiment analyzer\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment_roberta(tweet):\n",
    "    # Apply RoBERTa-based sentiment analysis\n",
    "    result = sentiment_pipe(tweet)[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "\n",
    "    # Set scores based on the label\n",
    "    positive_score_llm = score if label == \"LABEL_2\" else 0\n",
    "    negative_score_llm = score if label == \"LABEL_0\" else 0\n",
    "    neutral_score_llm = score if label == \"LABEL_1\" else 0\n",
    "\n",
    "    # Compound sentiment score for LLM (-1 to 1)\n",
    "    sentiment_score_llm = positive_score_llm - negative_score_llm\n",
    "\n",
    "    return sentiment_score_llm, negative_score_llm, neutral_score_llm, positive_score_llm\n",
    "\n",
    "def classify_sentiment_vader(tweet):\n",
    "    # Normalize text and apply VADER sentiment analysis\n",
    "    normalized_tweet = unicodedata.normalize('NFKD', tweet)\n",
    "    vader_result = sentiment_analyzer.polarity_scores(normalized_tweet)\n",
    "\n",
    "    # Extract VADER scores\n",
    "    sentiment_score_vader = vader_result['compound']\n",
    "    negative_score_vader = vader_result['neg']\n",
    "    neutral_score_vader = vader_result['neu']\n",
    "    positive_score_vader = vader_result['pos']\n",
    "\n",
    "    return sentiment_score_vader, negative_score_vader, neutral_score_vader, positive_score_vader\n",
    "\n",
    "def analyze_tweet_sentiments(df):\n",
    "    # Sample 10 tweets per day and calculate sentiment scores for both LLM and VADER\n",
    "    sample = df.sample(n=min(10, len(df)), random_state=1)  # random_state for reproducibility\n",
    "\n",
    "    # Apply RoBERTa-based sentiment analysis and VADER\n",
    "    llm_scores = sample['Tweet'].apply(classify_sentiment_roberta)\n",
    "    vader_scores = sample['Tweet'].apply(classify_sentiment_vader)\n",
    "\n",
    "    # Convert results to DataFrame and compute averages for each score\n",
    "    llm_scores_df = pd.DataFrame(llm_scores.tolist(), columns=['Sentiment Score LLM', 'Negative LLM', 'Neutral LLM', 'Positive LLM'])\n",
    "    vader_scores_df = pd.DataFrame(vader_scores.tolist(), columns=['Sentiment Score VADER', 'Negative VADER', 'Neutral VADER', 'Positive VADER'])\n",
    "\n",
    "    # Concatenate LLM and VADER score DataFrames and return the mean of each column\n",
    "    combined_df = pd.concat([llm_scores_df, vader_scores_df], axis=1)\n",
    "    return combined_df.mean()\n",
    "\n",
    "# Apply sampling and sentiment analysis, then group by date and company\n",
    "df_tweets_stock_daily = df_tweets_stock.groupby(['Date_day', 'Company Name', 'Stock Name']).apply(analyze_tweet_sentiments).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging Tweet Sentiment Data with Stock Price Data**\n",
    "Preparing the final dataset by merging the processed tweet sentiment data with the preprocessed stock price data based on the dates. This creates a unified dataset that combines both sentiment and stock price movements, enabling further analysis and the development of the time series models.\n",
    "\n",
    "#### **Resulting Dataset**\n",
    "\n",
    "- **Merged Dataset**: The resulting dataset, `final_df`, contains stock price data (e.g., closing price, volume) alongside tweet sentiment scores and labels.\n",
    "- **Preview of Data**: The `head()` function is used to display the first few rows of the final dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_stock['Date'] = pd.to_datetime(df_tweets_stock['Date'])\n",
    "df_tweets_stock['Date_day'] = df_tweets_stock['Date'].dt.date\n",
    "df_prices = pd.read_csv('../data/preprocessed_stock_data.csv')\n",
    "df_price_stock_daily = df_prices[df_prices['Stock Name'] == stock_name]\n",
    "df_price_stock_daily['Date'] = pd.to_datetime(df_price_stock_daily['Date'])\n",
    "df_price_stock_daily['Date_day'] = df_price_stock_daily['Date'].dt.date\n",
    "final_df = df_price_stock_daily.merge(df_tweets_stock_daily, how=\"inner\", on= [\"Date_day\",\"Stock Name\"]) \n",
    "final_df = final_df.drop(columns=['Stock Name', 'Date'])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of feature sets**\n",
    "\n",
    "Evaluating the performance of three models—**ARIMA/VAR**, **XGBoost**, and **LSTM**—across multiple feature sets. Each feature set incorporates different combinations of sentiment scores and stock price data to understand their impact on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature Sets**\n",
    "The feature sets used for evaluation include:\n",
    "1. Control set: `[\"Adj Close\"]` (only stock prices).\n",
    "2. Sentiment scores from various methods:\n",
    "   - **VADER**: Positive, Neutral, Negative, and Compound Sentiment Scores.\n",
    "   - **ZeroShot**: Positive, Neutral, Negative, and Compound Sentiment Scores.\n",
    "\n",
    "Each feature set is iteratively passed through both models for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Implementations**\n",
    "\n",
    "1. **ARIMA/VAR Model**:\n",
    "   - For single-feature sets (e.g., `[\"Adj Close\"]`), an ARIMA model is applied to predict future stock prices.\n",
    "   - For multi-feature sets, a VAR (Vector AutoRegression) model is used to account for interactions between features.\n",
    "   - Forecasting is done for the test set.\n",
    "\n",
    "2. **XGBoost with Lagged Variables**:\n",
    "   - Lagged variables are created for each feature in the set, representing prior values over a window of 4 time steps.\n",
    "   - Features and target values (`Adj Close`) are split into training and testing sets.\n",
    "   - The **XGBoost Regressor** is trained on the lagged data and used for predictions.\n",
    "\n",
    "3. **LSTM Model**:\n",
    "   - Data is normalized using a MinMaxScaler to scale values between 0 and 1.\n",
    "   - A sequence generator creates rolling sequences of the past 10 time steps for each feature.\n",
    "   - A multi-layer **LSTM network** with a dropout layer and dense output layer is defined and trained on the sequences.\n",
    "   - Predictions are generated for the test set and scaled back to the original range.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Evaluation**\n",
    "A custom evaluation function computes the following metrics for each model and feature set:\n",
    "- **RMSE (Root Mean Squared Error)**: Measures the average prediction error magnitude.\n",
    "- **MAE (Mean Absolute Error)**: Measures the average absolute prediction error.\n",
    "\n",
    "These metrics are appended to a `results` list for all models and feature sets.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Results Overview**\n",
    "The results coupled with an extensive analysis and discussion which provides justification and reasoning has been mentioned in the original research paper titled  _\"Using LLM to Predict Stock Price: A Hybrid Model Combining Social Media Sentiment and Market Data\"_.\n",
    "\n",
    "Images displaying previous results:\n",
    "\n",
    "* [Results for TSLA](../results/RMSE%20Table/Results%20TSLA.png)\n",
    "* [Results for AAPL](../results/RMSE%20Table/Results%20AAPL.png)\n",
    "* [Results for F](../results/RMSE%20Table/Results%20F.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\n",
    "    [\"Adj Close\"], # control set\n",
    "    [\"Adj Close\", \"Sentiment Score Roberta\"],\n",
    "    [\"Adj Close\", \"Sentiment Score ZeroShot\"],\n",
    "    [\"Adj Close\", \"Sentiment Score VADER\"],\n",
    "    [\"Adj Close\", \"Negative ZeroShot\"],\n",
    "    [\"Adj Close\", \"Neutral ZeroShot\"],\n",
    "    [\"Adj Close\", \"Positive ZeroShot\"],\n",
    "    [\"Adj Close\", \"Negative VADER\"],\n",
    "    [\"Adj Close\", \"Neutral VADER\"],\n",
    "    [\"Adj Close\", \"Positive VADER\"],\n",
    "]\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, feature_set):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return {\"Model\": model_name, \"Feature Set\": feature_set, \"RMSE\": rmse, \"MAE\": mae}\n",
    "\n",
    "results = []\n",
    "def arima_var_model(train, test, feature_set):\n",
    "    if len(feature_set) == 1:  \n",
    "        model = ARIMA(train[feature_set[0]], order=(5, 2, 0))  \n",
    "        model_fit = model.fit()\n",
    "        y_pred = model_fit.forecast(steps=len(test))\n",
    "        return pd.Series(y_pred, index=test.index)\n",
    "    else:  \n",
    "        model = VAR(train[feature_set])\n",
    "        model_fit = model.fit(5)  \n",
    "        y_pred = model_fit.forecast(train[feature_set].values[-5:], steps=len(test))\n",
    "        y_pred = pd.DataFrame(y_pred, index=test.index, columns=feature_set)\n",
    "        return y_pred['Adj Close']\n",
    "\n",
    "# XGBoost with Lagged Variables\n",
    "def xgboost_model(train, test, feature_set):\n",
    "    train = train.select_dtypes(include=[np.number])\n",
    "    test = test.select_dtypes(include=[np.number])\n",
    "\n",
    "    for lag in range(1, 5):\n",
    "        for feature in feature_set:\n",
    "            train[f\"{feature}_lag{lag}\"] = train[feature].shift(lag)\n",
    "            test[f\"{feature}_lag{lag}\"] = test[feature].shift(lag)\n",
    "    train.dropna(inplace=True)  \n",
    "\n",
    "    X_train, y_train = train.drop('Adj Close', axis=1), train['Adj Close']\n",
    "    X_test, y_test = test.drop('Adj Close', axis=1), test['Adj Close']\n",
    "\n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, enable_categorical=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# LSTM Model with Parameter Tuning\n",
    "def lstm_model(train, test, feature_set):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train[feature_set + ['Adj Close']])\n",
    "    test_scaled = scaler.transform(test[feature_set + ['Adj Close']])\n",
    "    sequence_length = 10  \n",
    "    generator = TimeseriesGenerator(train_scaled[:, :-1], train_scaled[:, -1], length=sequence_length, batch_size= sequence_length)\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, activation='relu', return_sequences=True, input_shape=(sequence_length, len(feature_set))))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(generator, epochs=50, verbose=0)\n",
    "    test_generator = TimeseriesGenerator(test_scaled[:, :-1], test_scaled[:, -1], length=sequence_length, batch_size=1)\n",
    "    y_pred_scaled = model.predict(test_generator)\n",
    "    y_pred = scaler.inverse_transform(np.concatenate([test_scaled[sequence_length:, :-1], y_pred_scaled], axis=1))[:, -1]\n",
    "    return pd.Series(y_pred, index=test.index[sequence_length:]), sequence_length\n",
    "\n",
    "\n",
    "# Loop through each feature set \n",
    "for feature_set in feature_sets:\n",
    "    train = final_df.iloc[:int(0.8 * len(final_df))].copy()\n",
    "    test = final_df.iloc[int(0.8 * len(final_df)):].copy()\n",
    "\n",
    "    # ARIMA Model for Linear Auto Regression\n",
    "    y_pred = arima_var_model(train, test, feature_set)\n",
    "    results.append(evaluate_model(test['Adj Close'], y_pred, \"ARIMA/VAR\", feature_set))\n",
    "\n",
    "    # XGBoost with Lagged Variables\n",
    "    y_pred= xgboost_model(train, test, feature_set)\n",
    "    results.append(evaluate_model(test['Adj Close'], y_pred, \"XGBoost\", feature_set))\n",
    "\n",
    "    # LSTM \n",
    "    y_pred, sequence_len  = lstm_model(train, test, feature_set)\n",
    "    results.append(evaluate_model(test.iloc[sequence_len:]['Adj Close'], y_pred, \"LSTM\", feature_set))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['Feature Set'] = results_df['Feature Set'].apply(lambda x: ', '.join(x))\n",
    "pivot_df = results_df.pivot(index='Feature Set', columns='Model', values=['RMSE', 'MAE']).reset_index().sort_values('Feature Set')\n",
    "pivot_df.columns = [f\"{metric}_{model}\" for metric, model in pivot_df.columns]\n",
    "print(pivot_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation Heatmaps**\n",
    "A correlation heatmap is a visualization displaying the correlation coefficients (r) that variables have with each other. These values range from -1 to 1:\n",
    "* **1** means a perfect positive correlation: as one variable increases, the other increases in a perfectly linear fashion.\n",
    "* **0** means no correlation: changes in one variable do not predict changes in the other variable.\n",
    "* **-1** means a perfect negative correlation: as one variable increases, the other decreases in a perfectly linear fashion.\n",
    "\n",
    "This is particularly useful for examining how different models' accuracies are related to sentiment and proving the hypothesis of the research paper.\n",
    "\n",
    "#### **Results Overview**\n",
    "Images displaying previous results:\n",
    "* [Results for TSLA](../results/Correlation%20Heatmaps/Correlation%20Heatmap%20TSLA.png)\n",
    "* [Results for AAPL](../results/Correlation%20Heatmaps/Correlation%20Heatmap%20AAPL.png)\n",
    "* [Results for F](../results/Correlation%20Heatmaps/Correlation%20Heatmap%20F.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_columns = [col for col in pivot_df.columns if col.startswith('RMSE_')]\n",
    "rmse_df = pivot_df[rmse_columns]\n",
    "accuracy_df = 100 - rmse_df\n",
    "accuracy_df.columns = [col.replace('RMSE_', '') for col in accuracy_df.columns]\n",
    "correlation_matrix = accuracy_df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Models with Sentiment')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
